{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import trange, tqdm\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA Availability\n",
    "device = \"\"\n",
    "def check_for_CUDA():\n",
    "    print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available() == False:\n",
    "        device = torch.device(\"cpu\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        cuda_id = torch.cuda.current_device() # Storing ID of current CUDA device\n",
    "        print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "        device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? False\n"
     ]
    }
   ],
   "source": [
    "check_for_CUDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set basic parameters to solve the Quadratic Entropy Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n = 10000 # probability vectors in \\R^n\n",
    "epsilon = 1 # regularization parameter\n",
    "max_iter = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(coupling_matrices, filenames):\n",
    "    for (matrix, filename) in zip(coupling_matrices, filenames):\n",
    "        torch.save(matrix, filename+\".pt\")\n",
    "\n",
    "def load_result(filenames):\n",
    "    for filename in filenames:\n",
    "        torch.load(filename+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize two marginal probability vectors $a \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-5, 5, n)\n",
    "\n",
    "mu_1 = norm.pdf(x, loc=0, scale=0.4)\n",
    "mu_2 = 0.5*norm.pdf(x, loc=-2, scale=0.2) + 0.5*norm.pdf(x, loc=1, scale=0.5)\n",
    "mu_3 = norm.pdf(x, loc=2, scale=0.6)\n",
    "mu_4 = norm.pdf(x, loc=3, scale=0.8)\n",
    "mu_5 = norm.pdf(x, loc=2.75, scale=0.7)\n",
    "\n",
    "mu_1 = torch.from_numpy(mu_1 / mu_1.sum())\n",
    "mu_2 = torch.from_numpy(mu_2 / mu_2.sum())\n",
    "mu_3 = torch.from_numpy(mu_3 / mu_3.sum())\n",
    "mu_4 = torch.from_numpy(mu_4 / mu_4.sum())\n",
    "mu_5 = torch.from_numpy(mu_5 / mu_5.sum())\n",
    "\n",
    "def plot_measures(x,mu_1,mu_2,mu_3,mu_4,mu_5):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, mu_1, label=r'$\\mu_1$', color='blue')\n",
    "    plt.plot(x, mu_2, label=r'$\\mu_2$', color='green')\n",
    "    plt.plot(x, mu_3, label=r'$\\mu_3$', color='red')\n",
    "    plt.plot(x, mu_4, label=r'$\\mu_4$', color='orange')\n",
    "    plt.plot(x, mu_5, label=r'$\\mu_5$', color='purple')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result([mu_1,mu_2,mu_3,mu_4,mu_5],[\"mu_1\",\"mu_2\",\"mu_3\",\"mu_4\",\"mu_5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize cost matrix $C \\in \\mathbb{R}_{+}^{n\\times n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean cost computes the Euclidean distance between two coordinates\n",
    "def compute_euclidean_cost(n):\n",
    "    print(\"Compute Euclidean Cost...\")\n",
    "    # Initialize cost matrix\n",
    "    x = torch.arange(n, dtype=torch.float64) # vector in \\R^n of the form [1,...,n]\n",
    "    C = ot.dist(x.reshape((n,1)), x.reshape((n,1))) # Euclidean metric as a cost function\n",
    "    return C/C.max() # normalize the cost\n",
    "\n",
    "# Weak Coulomb cost sets relatively large real value for diagonal entries\n",
    "def compute_weak_coulomb_cost(n, N=2, batch_size=5):\n",
    "    print(\"Computing Weak Coulomb Cost...\")\n",
    "    indices = generate_combinations_batched(n, N, batch_size) # Generate all index combinations in batches    \n",
    "    shape = (n,) * N # Initialize an N-dimensional tensor of size n in each dimension\n",
    "    C = torch.zeros(shape)\n",
    "    for index in indices: # Compute the Coulomb cost for each combination of indices\n",
    "        total_cost = 0\n",
    "        for i in range(N):\n",
    "            for j in range(i + 1, N):\n",
    "                diff = torch.abs(index[i] - index[j])\n",
    "                if diff != 0:\n",
    "                    total_cost += 1 / diff.item()  # Ensure diff is a scalar\n",
    "                else:\n",
    "                    total_cost += float(\"inf\")\n",
    "        C[tuple(index)] = total_cost # Assign the computed cost to the corresponding element in the matrix\n",
    "    return (C + C.T + torch.diag(n*torch.ones(n)))/n\n",
    "\n",
    "# Helper function for sliced batches\n",
    "def generate_combinations_batched(n, N, batch_size=10000):\n",
    "    indices = torch.tensor([], dtype=torch.long)\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        batch_indices = torch.combinations(torch.arange(start, end), r=N)\n",
    "        indices = torch.cat((indices, batch_indices), dim=0)\n",
    "    return indices\n",
    "\n",
    "# Strong Coulomb cost sets diagonal entires to be positive infinity\n",
    "def compute_strong_coulomb_cost(n, N=2, batch_size=5):\n",
    "    print(\"Computing Strong Coulomb Cost...\")\n",
    "    indices = generate_combinations_batched(n, N, batch_size) # Generate all index combinations in batches    \n",
    "    shape = (n,) * N # Initialize an N-dimensional tensor of size n in each dimension\n",
    "    C = torch.zeros(shape)\n",
    "    for index in indices: # Compute the Coulomb cost for each combination of indices\n",
    "        total_cost = 0\n",
    "        for i in range(N):\n",
    "            for j in range(i + 1, N):\n",
    "                diff = torch.abs(index[i] - index[j])\n",
    "                if diff != 0:\n",
    "                    total_cost += 1 / diff.item()  # Ensure diff is a scalar\n",
    "                else:\n",
    "                    total_cost += float(\"inf\")\n",
    "        C[tuple(index)] = total_cost # Assign the computed cost to the corresponding element in the matrix\n",
    "    return C + C.T + torch.diag(torch.ones(n) * float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_matrix(matrix):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.inferno, extent=(0.5,np.shape(matrix)[0]+0.5,0.5,np.shape(matrix)[1]+0.5))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Euclidean Cost...\n",
      "Computing Strong Coulomb Cost...\n",
      "Computing Weak Coulomb Cost...\n"
     ]
    }
   ],
   "source": [
    "# Generate cost matrices\n",
    "C_euc = compute_euclidean_cost(n)\n",
    "C_scou = compute_strong_coulomb_cost(n,batch_size=1)\n",
    "C_wcou = compute_weak_coulomb_cost(n,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot each cost matrix with respect to different governing principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coupling_matrices(matrices, titles, save=False):\n",
    "\n",
    "  if len(matrices) == 1:\n",
    "      fix, axes = plt.subplots(1, 1)\n",
    "  elif len(matrices) == 2:\n",
    "      # Create a figure and subplots\n",
    "      fig, axes = plt.subplots(1, 2, figsize=(14,14)) # Adjust figsize\n",
    "  elif len(matrices) == 3:\n",
    "      # Create a figure and subplots\n",
    "      fig, axes = plt.subplots(1, 3, figsize=(14,14))  # Adjust figsize for better visualization\n",
    "  elif len(matrices) == 4:\n",
    "      # Create a figure and subplots\n",
    "      fig, axes = plt.subplots(2, 2, figsize=(14,14)) # Adjust figsize\n",
    "  else:\n",
    "      print(\"Invalid input\")\n",
    "\n",
    "  # Loop through subplots and plot each matrix\n",
    "  axes = axes.flatten()\n",
    "  for i, matrix in enumerate(matrices):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.inferno, extent=(0.5, np.shape(matrix)[0] + 0.5, 0.5, np.shape(matrix)[1] + 0.5))\n",
    "    ax.set_title(titles[i])  # Add title to each subplot (optional)\n",
    "  # Adjust layout (optional)\n",
    "  fig.colorbar(im, ax=axes, shrink=0.5, location=\"bottom\")\n",
    "  plt.show()\n",
    "\n",
    "  if save == True:\n",
    "      plt.savefig(fig, f\"matrices_plot_{\"_\".join(titles)}.png\")\n",
    "      print(\"Plot saved as: \"+f\"matrices_plot_{\"_\".join(titles)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_coupling_matrices([C_euc, C_wcou, C_scou], [\"Euclidean\", \"Weak Coulomb\", \"Strong Coulomb\"])\n",
    "\n",
    "print(C_wcou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_GPU(args):\n",
    "    print(\"Transferring data to CUDA GPU...\")\n",
    "    for var in tqdm(args):\n",
    "        var.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_quadratic_cyclic_projection(C: torch.Tensor, a: torch.Tensor, b: torch.Tensor, epsilon: float, num_iter: int = 50000,\n",
    "                                         convergence_error: float = 1e-8, log=False) -> torch.Tensor:\n",
    "\n",
    "    print(\"Cyclic Projection\")\n",
    "    \n",
    "    n = a.size()[0]\n",
    "    m = b.size()[0]\n",
    "    f = torch.zeros_like(a)\n",
    "    g = torch.zeros_like(b)\n",
    "\n",
    "    # Use CUDA if possible\n",
    "    transfer_to_GPU([C,a,b,epsilon,n,m,f,g])\n",
    "    \n",
    "    for it in trange(num_iter):\n",
    "        f_prev = f\n",
    "        g_prev = g\n",
    "        rho = -(f.expand_as(C.T).T + g.expand_as(C) - C).clamp(max=0)\n",
    "        f = (epsilon * a - (rho + g.expand_as(C) - C).sum(1)) / m\n",
    "        g = (epsilon * b - (rho + f.expand_as(C.T).T - C).sum(0)) / n\n",
    "        f_diff = (f_prev - f).abs().sum()\n",
    "        g_diff = (g_prev - g).abs().sum()\n",
    "        if log:\n",
    "            print(f\"Iteration {it}\")\n",
    "            print(f\"f_diff {f_diff}\")\n",
    "            print(f\"g_diff {g_diff}\")\n",
    "        if f_diff < convergence_error and g_diff < convergence_error:\n",
    "            break\n",
    "\n",
    "    cyclic_projection = ((f.expand_as(C.T).T + g.expand_as(C) - C).clamp(min=0) / epsilon).cpu() # Retrieve result to CPU\n",
    "    \n",
    "    return cyclic_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_quadratic_gradient_descent(C: torch.Tensor, a: torch.Tensor, b: torch.Tensor, epsilon: float, num_iter: int = 50000,\n",
    "                                        convergence_error: float = 1e-8, log=False) -> torch.Tensor:\n",
    "\n",
    "    print(\"Gradient Descent\")\n",
    "    \n",
    "    n = a.size()[0]\n",
    "    m = b.size()[0]\n",
    "    f = torch.zeros_like(a)\n",
    "    g = torch.zeros_like(b)\n",
    "    step = 1.0 / (m + n)\n",
    "\n",
    "    # Use CUDA if possible\n",
    "    transfer_to_GPU([C,a,b,epsilon,n,m,f,g])\n",
    "\n",
    "    for it in trange(num_iter):\n",
    "        f_prev = f.clone()\n",
    "        g_prev = g.clone()\n",
    "\n",
    "        P = (f.expand_as(C.T).T + g.expand_as(C) - C).clamp(min=0) / epsilon\n",
    "\n",
    "        f -= step * epsilon * (P.sum(1) - a)\n",
    "        g -= step * epsilon * (P.sum(0) - b)\n",
    "\n",
    "        f_diff = (f_prev - f).abs().sum()\n",
    "        g_diff = (g_prev - g).abs().sum()\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iteration {it}\")\n",
    "            print(f\"f_diff {f_diff}\")\n",
    "            print(f\"g_diff {g_diff}\")\n",
    "\n",
    "        if f_diff < convergence_error and g_diff < convergence_error:\n",
    "            break\n",
    "\n",
    "    gradient_descent = ((f.expand_as(C.T).T + g.expand_as(C) - C).clamp(min=0) / epsilon).cpu() # Retrieve result to CPU\n",
    "    \n",
    "    return gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_quadratic_fixed_point_iteration(C: torch.Tensor, a: torch.Tensor, b: torch.Tensor, epsilon: float, num_iter: int = 50000,\n",
    "                                             convergence_error: float = 1e-8, log=False) -> torch.Tensor:\n",
    "\n",
    "    print(\"Fixed Point Iteration\")\n",
    "    \n",
    "    n = a.size()[0]\n",
    "    m = b.size()[0]\n",
    "    f = torch.zeros_like(a)\n",
    "    g = torch.zeros_like(b)\n",
    "\n",
    "    # Use CUDA if possible\n",
    "    transfer_to_GPU([C,a,b,epsilon,n,m,f,g])\n",
    "\n",
    "    for it in trange(num_iter):\n",
    "        f_prev = f.clone()\n",
    "        g_prev = g.clone()\n",
    "\n",
    "        P = (f.expand_as(C.T).T + g.expand_as(C) - C).clamp(min=0) / epsilon\n",
    "        v = - epsilon * (P.sum(1) - a)\n",
    "        f += (v - v.sum() / (2 * n)) / m\n",
    "        u = - epsilon * (P.sum(0) - b)\n",
    "        g += (u - u.sum() / (2 * m)) / n\n",
    "\n",
    "        f_diff = (f_prev - f).abs().sum()\n",
    "        g_diff = (g_prev - g).abs().sum()\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iteration {it}\")\n",
    "            print(f\"f_diff {f_diff}\")\n",
    "            print(f\"g_diff {g_diff}\")\n",
    "\n",
    "        if f_diff < convergence_error and g_diff < convergence_error:\n",
    "            break\n",
    "\n",
    "    fixed_point_iteration = ((f.expand_as(C.T).T + g.expand_as(C) - C).clamp(min=0) / epsilon).cpu()\n",
    "    \n",
    "    return fixed_point_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_quadratic_nesterov_gradient_descent(C: torch.Tensor, a: torch.Tensor, b: torch.Tensor, epsilon: float, num_iter: int = 50000,\n",
    "                                                 convergence_error: float = 1e-8, log=False) -> torch.Tensor:\n",
    "\n",
    "    print(\"Nesterov Gradient Descent\")\n",
    "    \n",
    "    n = a.size()[0]\n",
    "    m = b.size()[0]\n",
    "    f = torch.zeros_like(a)\n",
    "    g = torch.zeros_like(b)\n",
    "    step = 1.0 / (m + n)\n",
    "\n",
    "    # Use CUDA if possible\n",
    "    transfer_to_GPU([C,a,b,epsilon,n,m,f,g])\n",
    "\n",
    "    f_previous = f\n",
    "    g_previous = g\n",
    "\n",
    "    for it in trange(num_iter):\n",
    "        f_p = f + n * (f - f_previous) / (n + 3)\n",
    "        g_p = g + n * (g - g_previous) / (n + 3)\n",
    "\n",
    "        P = (f_p.expand_as(C.T).T\n",
    "             + g_p.expand_as(C) - C).clamp(min=0) / epsilon\n",
    "\n",
    "        f_new = f_p - step * epsilon * (P.sum(1) - a)\n",
    "        g_new = g_p - step * epsilon * (P.sum(0) - b)\n",
    "\n",
    "        f_diff = (f_previous - f_new).abs().sum()\n",
    "        g_diff = (g_previous - g_new).abs().sum()\n",
    "\n",
    "        f_previous = f\n",
    "        g_previous = g\n",
    "\n",
    "        f = f_new\n",
    "        g = g_new\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iteration {it}\")\n",
    "            print(f\"f_diff {f_diff}\")\n",
    "            print(f\"g_diff {g_diff}\")\n",
    "\n",
    "        if f_diff < convergence_error and g_diff < convergence_error:\n",
    "            break\n",
    "\n",
    "    nesterov_gradient_descent = ((f.expand_as(C.T).T + g.expand_as(C) - C).clamp(min=0) / epsilon).cpu()\n",
    "    \n",
    "    return nesterov_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_quadratic_nesterov_gradient_descent3(cost: torch.Tensor, marg1: torch.Tensor, marg2: torch.Tensor, marg3: torch.Tensor,\n",
    "                                                  epsilon: float, num_iter: int = 50000, convergence_error: float = 1e-8, log=False) -> torch.Tensor:\n",
    "    \n",
    "    print(\"Nesterov Gradient Descent 3 marginals\")\n",
    "    \n",
    "    n1 = marg1.size()[0]\n",
    "    n2 = marg2.size()[0]\n",
    "    n3 = marg3.size()[0]\n",
    "    p1 = torch.zeros_like(marg1)\n",
    "    p2 = torch.zeros_like(marg2)\n",
    "    p3 = torch.zeros_like(marg3)\n",
    "    step = 1.0 / (n1 + n2 + n3)\n",
    "    p1_prev = p1\n",
    "    p2_prev = p2\n",
    "    p3_prev = p3\n",
    "\n",
    "    # Use CUDA if possible\n",
    "    transfer_to_GPU([cost,marg1,marg2,marg3,epsilon,n1,n2,n3,p1,p2,p3,p1_prev,p2_prev,p3_prev])\n",
    "\n",
    "    for it in trange(num_iter):\n",
    "        p1_p = p1 + n1 * (p1 - p1_prev) / (n1 + 3)\n",
    "        p2_p = p2 + n2 * (p2 - p2_prev) / (n2 + 3)\n",
    "        p3_p = p3 + n3 * (p3 - p3_prev) / (n3 + 3)\n",
    "\n",
    "        P = (p1.expand_as(cost.T).T\n",
    "             + p2.expand_as(cost.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "             + p3.expand_as(cost) - cost).clamp(min=0) / epsilon\n",
    "\n",
    "        p1_new = p1_p - step * epsilon * (P.sum((1, 2)) - marg1)\n",
    "        p2_new = p2_p - step * epsilon * (P.sum((0, 2)) - marg2)\n",
    "        p3_new = p3_p - step * epsilon * (P.sum((0, 1)) - marg3)\n",
    "\n",
    "        p1_diff = (p1_prev - p1_new).abs().sum()\n",
    "        p2_diff = (p2_prev - p2_new).abs().sum()\n",
    "        p3_diff = (p3_prev - p3_new).abs().sum()\n",
    "\n",
    "        p1_prev = p1\n",
    "        p2_prev = p2\n",
    "        p3_prev = p3\n",
    "\n",
    "        p1 = p1_new\n",
    "        p2 = p2_new\n",
    "        p3 = p3_new\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iteration {it}\")\n",
    "            print(f\"p1_diff {p1_diff}\")\n",
    "            print(f\"p2_diff {p2_diff}\")\n",
    "            print(f\"p3_diff {p3_diff}\")\n",
    "\n",
    "        if p1_diff < convergence_error and p2_diff < convergence_error \\\n",
    "                and p2_diff < convergence_error:\n",
    "            break\n",
    "\n",
    "    nesterov_gradient_descent3 = ((p1.expand_as(cost.T).T + p2.expand_as(cost.permute(0, 2, 1)).permute(0, 2, 1) + p3.expand_as(cost) - cost).clamp(min=0) / epsilon).cpu()\n",
    "    \n",
    "    return nesterov_gradient_descent3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclic Projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▎                                                             | 12360/50000 [53:22<2:42:33,  3.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m P_cyclic_euc \u001b[38;5;241m=\u001b[39m sinkhorn_quadratic_cyclic_projection(C_euc, mu_1, mu_2, epsilon) \u001b[38;5;66;03m# max iteration = 50000, convergence error 1e-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m P_grad_euc \u001b[38;5;241m=\u001b[39m sinkhorn_quadratic_gradient_descent(C_euc, mu_1, mu_2, epsilon) \u001b[38;5;66;03m# max iteration = 50000, convergence error 1e-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m P_fpi_euc \u001b[38;5;241m=\u001b[39m sinkhorn_quadratic_fixed_point_iteration(C_euc, mu_1, mu_2, epsilon) \u001b[38;5;66;03m# max iteration = 50000, convergence error 1e-8\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36msinkhorn_quadratic_cyclic_projection\u001b[1;34m(C, a, b, epsilon, num_iter, convergence_error, log)\u001b[0m\n\u001b[0;32m     15\u001b[0m rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(f\u001b[38;5;241m.\u001b[39mexpand_as(C\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m g\u001b[38;5;241m.\u001b[39mexpand_as(C) \u001b[38;5;241m-\u001b[39m C)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m f \u001b[38;5;241m=\u001b[39m (epsilon \u001b[38;5;241m*\u001b[39m a \u001b[38;5;241m-\u001b[39m (rho \u001b[38;5;241m+\u001b[39m g\u001b[38;5;241m.\u001b[39mexpand_as(C) \u001b[38;5;241m-\u001b[39m C)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m m\n\u001b[1;32m---> 17\u001b[0m g \u001b[38;5;241m=\u001b[39m (epsilon \u001b[38;5;241m*\u001b[39m b \u001b[38;5;241m-\u001b[39m (rho \u001b[38;5;241m+\u001b[39m f\u001b[38;5;241m.\u001b[39mexpand_as(C\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m C)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m n\n\u001b[0;32m     18\u001b[0m f_diff \u001b[38;5;241m=\u001b[39m (f_prev \u001b[38;5;241m-\u001b[39m f)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     19\u001b[0m g_diff \u001b[38;5;241m=\u001b[39m (g_prev \u001b[38;5;241m-\u001b[39m g)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "P_cyclic_euc = sinkhorn_quadratic_cyclic_projection(C_euc, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_grad_euc = sinkhorn_quadratic_gradient_descent(C_euc, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_fpi_euc = sinkhorn_quadratic_fixed_point_iteration(C_euc, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_nesterov_euc = sinkhorn_quadratic_nesterov_gradient_descent(C_euc, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the resulting matrices are doubly stochastic\n",
    "list(map(torch.sum,[P_cyclic_euc,P_grad_euc,P_fpi_euc,P_nesterov_euc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_coupling_matrices([P_cyclic_euc, P_grad_euc, P_fpi_euc, P_nesterov_euc],\n",
    "                       [\"Cyclic Projection\", \"Gradient Descent\", \"Fixed Point Iteration\", \"Nesterov Gradient Descent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test these algorithms for Weak Coulomb Cost Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_cyclic_wcou = sinkhorn_quadratic_cyclic_projection(C_wcou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_grad_wcou = sinkhorn_quadratic_gradient_descent(C_wcou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_fpi_wcou = sinkhorn_quadratic_fixed_point_iteration(C_wcou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_nesterov_wcou = sinkhorn_quadratic_nesterov_gradient_descent(C_wcou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the resulting matrices are doubly stochastic\n",
    "list(map(torch.sum,[P_cyclic_wcou, P_grad_wcou, P_fpi_wcou, P_nesterov_wcou]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_coupling_matrices([P_cyclic_wcou, P_grad_wcou, P_fpi_wcou, P_nesterov_wcou],\n",
    "                       [\"Cyclic Projection\", \"Gradient Descent\", \"Fixed Point Iteration\", \"Nesterov Gradient Descent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test these algorithms for Strong Coulomb Cost Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_cyclic_scou = sinkhorn_quadratic_cyclic_projection(C_scou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_grad_scou = sinkhorn_quadratic_gradient_descent(C_scou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_fpi_scou = sinkhorn_quadratic_fixed_point_iteration(C_scou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8\n",
    "P_nesterov_scou = sinkhorn_quadratic_nesterov_gradient_descent(C_scou, mu_1, mu_2, epsilon) # max iteration = 50000, convergence error 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the resulting matrices are doubly stochastic\n",
    "list(map(torch.sum,[P_cyclic_scou,P_grad_scou,P_fpi_scou,P_nesterov_scou]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_coupling_matrices([P_cyclic_scou, P_grad_scou, P_fpi_scou, P_nesterov_scou],\n",
    "                       [\"Cyclic Projection\", \"Gradient Descent\", \"Fixed Point Iteration\", \"Nesterov Gradient Descent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
